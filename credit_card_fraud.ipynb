{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction & Problem Definition\n",
    "\n",
    "Credit card fraud is a growing issue in the financial sector, costing billions in losses each year. This project aims to develop a machine learning classification model to predict fraudulent transactions.\n",
    "\n",
    "## Key Questions\n",
    "- Can we accurately classify transactions as fraudulent (Class = 1) or legitimate (Class = 0)?\n",
    "- Which features (e.g., transaction amount, PCA-transformed components) are most predictive of fraud?\n",
    "- How does the imbalance in the data effect the model's performance, and what techniques can mitigate this issue?\n",
    "\n",
    "The goal for this project is to try and solve a binary classification problem and extract actionable insights from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview\n",
    "\n",
    "The data set is sourced from Kaggle ([Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)). It contains transactions made by credit cards in September 2013 by European cardholders. The data set contains 31 columns and 284,807 rows. The features include:\n",
    "\n",
    "- Time: Seconds elapsed between each transaction and the first transaction in the dataset.\n",
    "- V1 to V28: Anonymized features obtained via PCA transformation, they capture the essential patterns in the data.\n",
    "- Amount: The transaction amount.\n",
    "- Class: The target variable where 0 represents a legitimate transaction and 1 indicates fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('creditcard.csv')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing the Data\n",
    "\n",
    "Pre-processing steps ensure that our data is clean and suitable for modeling. I will:\n",
    "\n",
    "- Inspect Data Quality: Check for missing or anomalous values.\n",
    "- Feature Scaling: Standardize the 'Amount' and 'Time' features for consistency.\n",
    "- Address Class Imbalance: Use techniques like oversampling and undersampling to balance the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First I am splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = data.drop('Class', axis=1)\n",
    "y = data['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will be using SMOTE (Synthetic Minority Oversampling Technique) to balance the dataset\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Original class distribution: \")\n",
    "print(y.value_counts())\n",
    "print(\"\\nResampled class distribution: \")\n",
    "print(y_resampled.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am checking for missing values and data types\n",
    "print(data.info())\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Standardizing the 'Amount' and 'Time' features using StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data[['Time','Amount']] = scaler.fit_transform(data[['Time','Amount']])\n",
    "\n",
    "# Displaying the summary statistics to verify the scaling\n",
    "data[['Time', 'Amount']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Class Distribution: Before and After Balancing\n",
    "\n",
    "Seeing such an imbalanced bar chart is expected. The dataset has around 284,315 legitimate transactions (Class = 0) and 492 fraudulent transactions (Class=1). This shows us the distribution of the original class distribution, but in order to train the classification model effectively, I will need to use SMOTE (Synthetic Minority Oversampling Technique) to balance the dataset. Below you can see before and after oversampling the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='Class', data=data)\n",
    "plt.title(\"Class Distribution Before SMOTE\")\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(x=y_resampled)\n",
    "plt.title(\"Class Distribution After SMOTE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Data With Visualizations\n",
    "\n",
    "Exploratory Data Analysis (EDA)\n",
    "\n",
    "Explore the dataset to understand the distribution of features and relationships. This includes:\n",
    "\n",
    "- Plotting histograms and boxplots to understand feature distributions.\n",
    "- Creating a correlation heatmap for PCA components.\n",
    "- Visualizing class imbalance and distributions of 'Amount' and 'Time'.\n",
    "\n",
    "These visualizations help identify key trends that will inform my modeling strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting 'Amount'\n",
    "\n",
    "First I have plotted the distribution of the 'Amount' feature using a histogram. Keep in mind that transaction amounts in credit card datasets are often heavily skewed (many small amounts, fewer large amounts). By visualizing the distribution, I can see how the data is spread out and whether there are extreme outliers, thus guiding my pre-processing decisions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution of the 'Amount' feature.\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['Amount'], bins=50, kde=True)\n",
    "plt.title(\"Distribution of Transaction Amounts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be thinking that something went wrong here but this is completely normal for financial transaction data. As I mentioned before, credit card data is known to be extremely skewed, with most transactions having relatively small amounts. As a result, by plotting a histogram on the near-original scale, I get a tall spike near zero and a long tail extending to higher values. In order to visualize the distribution in a more spread-out manner, I have applied a log transform. This way, the large range of values is compressed and you can see the shape of the distribution more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['log_Amount'] = np.log1p(data['Amount'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['log_Amount'], bins=50, kde=True)\n",
    "plt.title(\"Distribution of Log-Transformed Transaction Amounts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting 'Time'\n",
    "\n",
    "Next I used a boxplot to plot the 'Time' distribution. A boxplot shows the median, interquartile range, and overall spread of the data. For the 'Time' feature, this helps us to see how transactions are distributed throughout the time window captured by the dataset. If there are extreme values in this feature, the boxplot makes them easy to spot. Although 'Time' may not be as directly interpretable as a financial metric, examining it via a boxplot is still a valuable step in understanding the dataset's characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for the 'Time' feature.\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.boxplot(x=data['Time'])\n",
    "plt.title(\"Boxplot of Time Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see a fairly symmetric distribution with no extreme outliers. Given that I scaled the 'Time' feature for consistency, a boxplot with no apparent outliers and a relatively balanced box/whiskers simply indicates that the distribution does not have extreme deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Heatmap for PCA Features\n",
    "\n",
    "Althought Principal Component Analysis, or PCA, is designed to produce uncorrelated components, it's still valuable to verify this assumption visually. A correlation heatmap allows us to confirm orthogonality, check for anomolies, and reassure model assumptions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "pca_features = [col for col in data.columns if col.startswith('V')]\n",
    "sns.heatmap(data[pca_features].corr(), cmap=\"coolwarm\", annot=False)\n",
    "plt.title(\"Correlation Heatmap of PCA Components\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there is a strong red along the diagonal and dark blue elsewhere. This means that the components are effectively uncorrelated with one another, exactly what we want to see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model\n",
    "\n",
    "I will experiment with several classification algorithms:\n",
    "- Logistic Regression: Simple and interpretable, serving as a baseline.\n",
    "- Random Forest: Capture non-linear relationships and interactions.\n",
    "- Gradient Boosting: This may offer higher accuracy on imbalanced data.\n",
    "\n",
    "We want to compare with other models because we are working with imbalanced data.\n",
    "\n",
    "I will use a confusion matrix to visualize the classification results, as it directly shows how many predictions were correct or incorrect for each class.\n",
    "\n",
    "We want to improve recall without sacrificing too much precision, this is critical for applications like fraud detection, where missing a fraud case could be costly.\n",
    "\n",
    "\n",
    "Choosing the right threshold at the intersecion of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Precision-Recall curve to find the right threshold\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "lr_model = LogisticRegression(solver='saga', max_iter=1000, tol=1e-3,random_state=42)\n",
    "lr_model.fit(X_resampled, y_resampled)\n",
    "y_scores = lr_model.predict_proba(X_test)[:, 1]\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_scores)\n",
    "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "plt.title(\"Precision-Recall vs. Threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Logistic Regression model\n",
    "\n",
    "threshold = 0.7\n",
    "\n",
    "\n",
    "y_pred_adjusted = (y_scores >= threshold).astype(int)\n",
    "\n",
    "# Evaluating the model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_adjusted))\n",
    "print(\"ROC-AUC Score: \", roc_auc_score(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix?\n",
    "A confusion matrix is a tabular way of visualizing the performance of a classification model by comparing the true labels with the model’s predicted labels. For a binary classification problem, the confusion matrix has four main cells:\n",
    "\n",
    "- True Negatives (Top-Left): The model correctly predicts the negative (non-fraud, for example) class.\n",
    "- False Positives (Top-Right): The model incorrectly predicts the positive (fraud) class when it’s actually negative.\n",
    "- False Negatives (Bottom-Left): The model incorrectly predicts the negative class when it’s actually positive.\n",
    "- True Positives (Bottom-Right): The model correctly predicts the positive class.\n",
    "\n",
    "From these values, we can calculate key metrics such as precision, recall, F1-score, and accuracy, which give deeper insight into the chosen model’s strengths and weaknesses.\n",
    "- Precision (Positive Predictive Value): Measures how many of the predicted positive cases are actually positive.\n",
    "- Recall (Sensitivy or True Positivie Rate): Measures how many of the actual positive cases the model correctly identified.\n",
    "- F1-Score: Measures the overall proportion of the correctly classified instances (both positives and negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_adjusted)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Legitimate', 'Fraud'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix: Logistic Regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression: Interpreting the Results\n",
    "\n",
    "- Precision (Fraud) = TP / (TP + FP) = 55 / (55 + 99) ≈ 0.36\n",
    "\n",
    "- Recall (Fraud) = TP / (TP + FN) = 55 / (55 + 93) ≈ 0.37\n",
    "\n",
    "- Accuracy = (TN + TP) / (Total) = (85196 + 55) / 85443 ≈ 0.997\n",
    "\n",
    "High Accuracy, Low Recall\n",
    "\n",
    "- The model is great at identifying legitimate transactions (hence high accuracy), but it misses many fraud cases (63%).\n",
    "\n",
    "Low Precision\n",
    "\n",
    "- When the model does flag transactions as fraud, it’s only correct 36% of the time. This leads to some “false alarms.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_resampled, y_resampled)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"ROC-AUC Score: \", roc_auc_score(y_test, rf_model.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "disp_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf, display_labels=['Legitimate', 'Fraud'])\n",
    "disp_rf.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix: Random Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest: Interpreting the Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "negative_count = sum(y_train == 0)\n",
    "positive_count = sum(y_train == 1)\n",
    "ratio = negative_count / positive_count\n",
    "xgb_model = xgb.XGBClassifier(random_state=42, scale_pos_weight=1)\n",
    "\n",
    "xgb_model.fit(X_resampled, y_resampled)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "print(\"XGBoost Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, xgb_model.predict_proba(X_test)[:, 1]))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "disp_xgb = ConfusionMatrixDisplay(confusion_matrix=cm_xgb, display_labels=['Legitimate', 'Fraud'])\n",
    "disp_xgb.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix: XGBoost\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost: Interpreting Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storytelling: What have we learned?\n",
    "Summarize the entire process—from data exploration to model evaluation:\n",
    "\n",
    "- What did your visualizations reveal about the data structure and potential fraud patterns?\n",
    "- How did each model perform, and what trade-offs did you observe?\n",
    "- Were the initial questions answered? For example, were you able to identify significant features that predict fraud?\n",
    "\n",
    "\"In exploring the data, I discovered that while most transactions are legitimate, certain subtle patterns in the PCA-transformed features strongly indicate fraudulent behavior. The logistic regression model provided a baseline, but more complex models like Random Forests and XGBoost captured the intricacies of the data more effectively. These insights could help financial institutions design more robust fraud detection systems.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "([Kaggle - Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud))\n",
    "\n",
    "Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n",
    "\n",
    "Dal Pozzolo, Andrea; Caelen, Olivier; Le Borgne, Yann-Ael; Waterschoot, Serge; Bontempi, Gianluca. Learned lessons in credit card fraud detection from a practitioner perspective, Expert systems with applications,41,10,4915-4928,2014, Pergamon\n",
    "\n",
    "Dal Pozzolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi, Cesare; Bontempi, Gianluca. Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE transactions on neural networks and learning systems,29,8,3784-3797,2018,IEEE\n",
    "\n",
    "Dal Pozzolo, Andrea Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n",
    "\n",
    "Carcillo, Fabrizio; Dal Pozzolo, Andrea; Le Borgne, Yann-Aël; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information fusion,41, 182-194,2018,Elsevier\n",
    "\n",
    "Carcillo, Fabrizio; Le Borgne, Yann-Aël; Caelen, Olivier; Bontempi, Gianluca. Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics, 5,4,285-300,2018,Springer International Publishing\n",
    "\n",
    "Bertrand Lebichot, Yann-Aël Le Borgne, Liyun He, Frederic Oblé, Gianluca Bontempi Deep-Learning Domain Adaptation Techniques for Credit Cards Fraud Detection, INNSBDDL 2019: Recent Advances in Big Data and Deep Learning, pp 78-88, 2019\n",
    "\n",
    "Fabrizio Carcillo, Yann-Aël Le Borgne, Olivier Caelen, Frederic Oblé, Gianluca Bontempi Combining Unsupervised and Supervised Learning in Credit Card Fraud Detection Information Sciences, 2019\n",
    "\n",
    "Yann-Aël Le Borgne, Gianluca Bontempi Reproducible machine Learning for Credit Card Fraud Detection - Practical Handbook\n",
    "\n",
    "Bertrand Lebichot, Gianmarco Paldino, Wissam Siblini, Liyun He, Frederic Oblé, Gianluca Bontempi Incremental learning strategies for credit cards fraud detection, IInternational Journal of Data Science and Analytics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
